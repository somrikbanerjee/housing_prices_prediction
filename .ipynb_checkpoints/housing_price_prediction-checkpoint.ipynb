{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb3PSul8Dful"
   },
   "source": [
    "# Assignment: Advanced Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQduz_APD198"
   },
   "source": [
    "## 1. Introduction\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVbN2rGqDlNV"
   },
   "source": [
    "#### 1.1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0tGuu-WHHps"
   },
   "source": [
    "A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\n",
    "\n",
    "The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n",
    "\n",
    "The company wants to know:\n",
    "- Which variables are significant in predicting the price of a house, and\n",
    "- How well those variables describe the price of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HuxN3F3DnoC"
   },
   "source": [
    "#### 1.2. Business Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSymXBABHQkh"
   },
   "source": [
    "The goal is to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I48WZ6Z9DueT"
   },
   "source": [
    "#### 1.3. Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UfvkrPUHVVs"
   },
   "source": [
    "Since the price of a house is a continuous variable, this is a regression problem. The analysis will be conducted in the following steps:\n",
    "\n",
    "- Data cleaning and removal of unnecessary features\n",
    "- Exploratory data analysis and visualization\n",
    "- Data preprocessing and feature selection\n",
    "- Fitting a regression model on the data\n",
    "- Calculating the feature importances\n",
    "- Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDUuGMEfEBBA"
   },
   "source": [
    "## 2. Data Ingestion\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kwd_p55LCTK5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HdqUIb43F7b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdXGAX-nELUG"
   },
   "source": [
    "## 3. Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDN9_vrCFe4u"
   },
   "source": [
    "#### 3.1. Treating NULLs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex9v_yjVIKSf"
   },
   "source": [
    "First, we dive into the dataset to flag columns with a lot of NULL values, because they do not really add anything useful to our analysis. With the amount of data we have, it is smart to keep a tight grip on the number of NULLs we allow. Being too lenient would mean more columns filled with NULLs than we would want, compared to the total rows we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aXgsqW_PLqiO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC          1453\n",
       "MiscFeature     1406\n",
       "Alley           1369\n",
       "Fence           1179\n",
       "MasVnrType       872\n",
       "FireplaceQu      690\n",
       "LotFrontage      259\n",
       "GarageType        81\n",
       "GarageYrBlt       81\n",
       "GarageFinish      81\n",
       "GarageQual        81\n",
       "GarageCond        81\n",
       "BsmtFinType2      38\n",
       "BsmtExposure      38\n",
       "BsmtFinType1      37\n",
       "BsmtCond          37\n",
       "BsmtQual          37\n",
       "MasVnrArea         8\n",
       "Electrical         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, df.columns[(df.isnull().sum() / df.shape[0]) > 0]].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgKns29TMTvJ"
   },
   "source": [
    "We can straightaway remove the features with more than 1000 NULLs since they almost completely consume the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vfflQGr0OVOo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MasVnrType      872\n",
       "FireplaceQu     690\n",
       "LotFrontage     259\n",
       "GarageType       81\n",
       "GarageYrBlt      81\n",
       "GarageFinish     81\n",
       "GarageQual       81\n",
       "GarageCond       81\n",
       "BsmtExposure     38\n",
       "BsmtFinType2     38\n",
       "BsmtQual         37\n",
       "BsmtCond         37\n",
       "BsmtFinType1     37\n",
       "MasVnrArea        8\n",
       "Electrical        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis = 1, inplace = True)\n",
    "df.loc[:, df.columns[(df.isnull().sum() / df.shape[0]) > 0]].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaxJ28RdOy14"
   },
   "source": [
    "As per the data dictionary, many of these columns have an NA category. We may want to impute the missing values for those columns as NA, since NA itself implies the lack of that feature. For `Electrical`, since there is only one NULL, we can drop that row.\n",
    "\n",
    "For the basement features `BsmtExposure`, `BsmtQual` and `BsmtCond`, `BsmtFinType1` and `BsmtFinType2`, `TotalBsmtSF` being 0 would confirm the lack of a basement and we can safely assign the features as \"NA\". This would take care of many of the NULLs which are actually due to lack of a basement area and also anomalies where area is zero but the related features are not NA.\n",
    "\n",
    "Similarly, if `Fireplaces` = 0, then we can assign `FireplaceQu` as NA for those houses.\n",
    "\n",
    "Similarly, if `GarageArea` = 0, then the remaining garage features can be safely assigned as NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oNGIRNniOx6g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MasVnrType      871\n",
       "LotFrontage     259\n",
       "MasVnrArea        8\n",
       "BsmtExposure      1\n",
       "BsmtFinType2      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[~df['Electrical'].isna(), ]\n",
    "df.loc[df['TotalBsmtSF'] == 0, ['BsmtExposure', 'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']] = 'NA'\n",
    "df.loc[df['GarageArea'] == 0, ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']] = 'NA'\n",
    "df.loc[df['Fireplaces'] == 0, 'FireplaceQu'] = 'NA'\n",
    "df.loc[:, df.columns[(df.isnull().sum() / df.shape[0]) > 0]].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlTi4dk2W-Er"
   },
   "source": [
    "`MasVnrArea` can be imputed with 0 in case of NULLs since there are only 8 missing values. Consequently, we can assign `MasVnrType` as \"None\" for those houses.\n",
    "\n",
    "We can drop the houses with NULL `BsmtExposure` and `BsmtFinType2` at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-8YgwH8Rox3"
   },
   "source": [
    "`BsmtFinType1` and `BsmtFinType2` are ratings of Basement Finished area, which would depend on how much of the basement area is finished. Ideally, if we have `BsmtUnfSF` as non-zero, then the ratings would be \"Unf\", indicating \"unfinished\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D3yxumrIRofn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage    259\n",
       "MasVnrType       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[~df['BsmtFinType2'].isna(), ]\n",
    "df = df.loc[~df['BsmtExposure'].isna(), ]\n",
    "df['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n",
    "df.loc[df['MasVnrArea'] == 0, 'MasVnrType'] = 'None'\n",
    "\n",
    "df.loc[:, df.columns[(df.isnull().sum() / df.shape[0]) > 0]].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtCZ6YF3YBmE"
   },
   "source": [
    "Finally, we impute the `LotFrontage` feature by the global median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B086M8cSYqdx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MasVnrType    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = df['LotFrontage'].median()\n",
    "df['LotFrontage'] = df['LotFrontage'].fillna(m)\n",
    "df.loc[:, df.columns[(df.isnull().sum() / df.shape[0]) > 0]].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A09j7HFRbgt6"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "In this section, the areas of basement, garage, masonry or number of fireplaces being 0 implied that there was no basement or garage or masonry or fireplace. However, we cannot assume the converse to be true because it is possible for a house to not have a basement or garage or masonry or a fireplace but still have allocated area for future construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSbY8mfkcSm0"
   },
   "source": [
    "#### 3.2. Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGh9vXMAcVvX"
   },
   "source": [
    "Here we remove the columns that indicate some sort of ID, since they are identifiers and do not contribute to the analysis. We also remove the columns that have exactly 1 value, if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1s8RKKb0cfH5"
   },
   "outputs": [],
   "source": [
    "df.drop('Id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7DsvKE4Yc9qW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in df.columns if df[col].nunique() == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwVNRuU1cJhW"
   },
   "source": [
    "## 4. Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX5yHJGpdjCR"
   },
   "source": [
    "#### 4.1. Deriving Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJrT1WWSgW_p"
   },
   "source": [
    "There are 4 year columns, i.e. `YearBuilt`, `YearRemodAdd`, `GarageYrBlt` and `YrSold`. Year values themselves are not of much use, but we can surely calculate things such as the age of the house at the time of selling, the number of years since the last remodel, number of years since garage was built, etc.\n",
    "\n",
    "Here we shall create 4 new derived metrics:\n",
    "- `YrsAtSale` which indicates age of the house at the time of sale\n",
    "- `YrSinceRemod` which indicates how many years have passed since the last renovation at the time of sale\n",
    "- `GarageAge` which indicates the age of the garage. If it has not been built, it would be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jA6QI24hdivr"
   },
   "outputs": [],
   "source": [
    "df['YrsAtSale'] = df['YrSold'] - df['YearBuilt']\n",
    "df['YrSinceRemod'] = df['YrSold'] - df['YearRemodAdd']\n",
    "df['GarageAge'] = df.apply(lambda row: row['YrSold'] - int(row['GarageYrBlt']) if row['GarageYrBlt'] != 'NA' else 0, axis = 1)\n",
    "\n",
    "## Dropping the year variables now\n",
    "df.drop(['YrSold', 'GarageYrBlt', 'YearRemodAdd', 'YearBuilt'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M1wm5Ip5egW"
   },
   "source": [
    "#### 4.2. Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vRzQ09L5egX"
   },
   "source": [
    "Here we shall create dummies out of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hgSk1Lop5egX"
   },
   "outputs": [],
   "source": [
    "cat_vars = [\n",
    "    'MSSubClass',\n",
    "    'MSZoning',\n",
    "    'Street',\n",
    "    'LandContour',\n",
    "    'LotConfig',\n",
    "    'Neighborhood',\n",
    "    'Condition1',\n",
    "    'Condition2',\n",
    "    'BldgType',\n",
    "    'HouseStyle',\n",
    "    'RoofStyle',\n",
    "    'RoofMatl',\n",
    "    'Exterior1st',\n",
    "    'Exterior2nd',\n",
    "    'MasVnrType',\n",
    "    'Foundation',\n",
    "    'Heating',\n",
    "    'CentralAir',\n",
    "    'Electrical',\n",
    "    'GarageType',\n",
    "    'SaleType',\n",
    "    'SaleCondition'\n",
    "]\n",
    "\n",
    "num_vars = [c for c in df.columns if c not in cat_vars]\n",
    "\n",
    "for var in cat_vars:\n",
    "    dummies = pd.get_dummies(df[var], prefix = var, drop_first = True).astype(int)\n",
    "    df = pd.concat([df, dummies], axis = 1)\n",
    "    df.drop(var, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBULFGyRdqFd"
   },
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MvfukRX5egX"
   },
   "source": [
    "#### 4.3. Ordinal Variables\n",
    "\n",
    "Now we shall encode the ordinal variables into numeric such that their ordering is maintained. Note that the variables `OverallQual` and `OverallQual` are already ordinal and numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HjQx0p7RdpsN"
   },
   "outputs": [],
   "source": [
    "lotshape = {\n",
    "    'IR3': 0,\n",
    "    'IR2': 1,\n",
    "    'IR1': 2,\n",
    "    'Reg': 3\n",
    "}\n",
    "\n",
    "utilities = {\n",
    "    'ELO': 0,\n",
    "    'NoSeWa': 1,\n",
    "    'NoSewr': 2,\n",
    "    'AllPub': 3\n",
    "}\n",
    "\n",
    "landslope = {\n",
    "    'Sev': 0,\n",
    "    'Mod': 1,\n",
    "    'Gtl': 2\n",
    "}\n",
    "\n",
    "qual1 = {\n",
    "    'Po': 0,\n",
    "    'Fa': 1,\n",
    "    'TA': 2,\n",
    "    'Gd': 3,\n",
    "    'Ex': 4\n",
    "}\n",
    "\n",
    "qual2= {\n",
    "    'NA': 0,\n",
    "    'Po': 1,\n",
    "    'Fa': 2,\n",
    "    'TA': 3,\n",
    "    'Gd': 4,\n",
    "    'Ex': 5\n",
    "}\n",
    "\n",
    "bsmtexposure = {\n",
    "    'NA': 0,\n",
    "    'No': 1,\n",
    "    'Mn': 2,\n",
    "    'Av': 3,\n",
    "    'Gd': 4\n",
    "}\n",
    "\n",
    "bsmtfintype = {\n",
    "    'NA': 0,\n",
    "    'Unf': 1,\n",
    "    'LwQ': 2,\n",
    "    'Rec': 3,\n",
    "    'BLQ': 4,\n",
    "    'ALQ': 5,\n",
    "    'GLQ': 6\n",
    "}\n",
    "\n",
    "functional = {\n",
    "    'Sal': 0,\n",
    "    'Sev': 1,\n",
    "    'Maj2': 2,\n",
    "    'Maj1': 3,\n",
    "    'Mod': 4,\n",
    "    'Min2': 5,\n",
    "    'Min1': 6,\n",
    "    'Typ': 7\n",
    "}\n",
    "\n",
    "garagefinish = {\n",
    "    'NA': 0,\n",
    "    'Unf': 1,\n",
    "    'RFn': 2,\n",
    "    'Fin': 3\n",
    "}\n",
    "\n",
    "paveddrive = {\n",
    "    'N': 0,\n",
    "    'P': 1,\n",
    "    'Y': 2\n",
    "}\n",
    "\n",
    "df['LotShape'] = df['LotShape'].apply(lambda x: lotshape[x])\n",
    "df['Utilities'] = df['Utilities'].apply(lambda x: utilities[x])\n",
    "df['LandSlope'] = df['LandSlope'].apply(lambda x: landslope[x])\n",
    "df['ExterQual'] = df['ExterQual'].apply(lambda x: qual1[x])\n",
    "df['ExterCond'] = df['ExterCond'].apply(lambda x: qual1[x])\n",
    "df['BsmtQual'] = df['BsmtQual'].apply(lambda x: qual2[x])\n",
    "df['BsmtCond'] = df['BsmtCond'].apply(lambda x: qual2[x])\n",
    "df['BsmtExposure'] = df['BsmtExposure'].apply(lambda x: bsmtexposure[x])\n",
    "df['BsmtFinType1'] = df['BsmtFinType1'].apply(lambda x: bsmtfintype[x])\n",
    "df['BsmtFinType2'] = df['BsmtFinType2'].apply(lambda x: bsmtfintype[x])\n",
    "df['HeatingQC'] = df['HeatingQC'].apply(lambda x: qual1[x])\n",
    "df['KitchenQual'] = df['KitchenQual'].apply(lambda x: qual1[x])\n",
    "df['Functional'] = df['Functional'].apply(lambda x: functional[x])\n",
    "df['FireplaceQu'] = df['FireplaceQu'].apply(lambda x: qual2[x])\n",
    "df['GarageFinish'] = df['GarageFinish'].apply(lambda x: garagefinish[x])\n",
    "df['GarageQual'] = df['GarageQual'].apply(lambda x: qual2[x])\n",
    "df['GarageCond'] = df['GarageCond'].apply(lambda x: qual2[x])\n",
    "df['PavedDrive'] = df['PavedDrive'].apply(lambda x: paveddrive[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VmxD831D5egX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1457, 207)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IBHFrs9a5egX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1457 entries, 0 to 1459\n",
      "Columns: 207 entries, LotFrontage to SaleCondition_Partial\n",
      "dtypes: float64(2), int64(205)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HALEauqg5egX"
   },
   "source": [
    "#### 4.4. Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hCoB4GP5egX"
   },
   "source": [
    "Now that we have encoded all non-numeric features into numeric form, it is time to reduce the number of features to a more manageable size. First we shall split the data into training and test sets, then apply a recursive feature elimination model on the training set to get the set of most relevant features. Paraphrasing Pareto's Law, 80% of the variance should be explained by 20% of the features, which in this case should be 41 (20% of 206). But let us be a bit more conservative and cap the total features to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EEN-H77O5egX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "df_train, df_test = train_test_split(df, random_state = 42)\n",
    "df_train.loc[:, num_vars] = sc.fit_transform(df_train.loc[:, num_vars])\n",
    "df_test.loc[:, num_vars] = sc.transform(df_test.loc[:, num_vars])\n",
    "\n",
    "y_train = df_train.pop('SalePrice')\n",
    "y_test = df_test.pop('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTlNnWxp5egY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(df_train, y_train)\n",
    "sfc = SequentialFeatureSelector(estimator = lr, direction = 'backward')\n",
    "\n",
    "sfc.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrnoyMiB5egY"
   },
   "outputs": [],
   "source": [
    "cols_to_keep = df_train.columns[rfe.support_]\n",
    "# df_train = df_train.loc[:, cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c85AiXNn5egY"
   },
   "source": [
    "#### 4.5. Treating multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q30XMq8CF87j"
   },
   "source": [
    "Next, we treat any multicollinearity within the predictor variables. We shall use Variance Inflation Factor (VIF) to recursively identify the predictor having the highest multicollinearity with one or more predictors and drop it. We shall keep the tolerance threshold for VIF at 5, which is equivalent to ensuring no set of predictors have 80% or more linear dependency among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoAk9K_Z5egY"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "vif_df = pd.DataFrame({'features': df_train.columns}).reset_index()\n",
    "vif_df['score'] = vif_df.apply(lambda row: VIF(df_train, row['index']), axis = 1)\n",
    "\n",
    "while vif_df['score'].max() > 4:\n",
    "    ix = vif_df['score'].argmax()\n",
    "    f = vif_df.iloc[ix]['features']\n",
    "    sc = vif_df['score'].max()\n",
    "    print(f\"Removing {f},  VIF = {sc}\")\n",
    "    df_train.drop(f, axis = 1, inplace = True)\n",
    "    vif_df = pd.DataFrame({'features': df_train.columns}).reset_index()\n",
    "    vif_df['score'] = vif_df.apply(lambda row: VIF(df_train, row['index']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FccyKl8r5egY"
   },
   "outputs": [],
   "source": [
    "print(df_train.columns)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKX-1BeXApTz"
   },
   "outputs": [],
   "source": [
    "vif_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2_olVFgGo5t"
   },
   "source": [
    "Finally we have arrived at 38 predictor columns, which we shall now use for model building. Note that we did not use backward elimination or any feature elimination processes that use the significance of the beta coefficients. This is because we intend to perform regularization on the models going forward and unnecessary features will be reduced in importance (ridge) or removed completely (lasso) during the modelling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzCNQJ9ZHWFm"
   },
   "source": [
    "## 5. Data Modelling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.loc[:, df_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "params = {'alpha': np.arange(0, 10, 0.001)}\n",
    "\n",
    "ridge = GridSearchCV(\n",
    "    estimator = Ridge(),\n",
    "    param_grid = params,\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = 3,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "ridge.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge.best_params_)\n",
    "print(ridge.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now test the performance of this model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ridge = ridge.predict(df_test)\n",
    "print('R-squared for test set:', r2_score(y_test, y_pred_ridge))\n",
    "print('RMSE for test set:', root_mean_squared_error(y_test, y_pred_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results do not look promising at 65% R-squared score. We shall now test the residuals to see whether or not they follow the assumptions of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Residual Analysis I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = ridge.predict(df_train)\n",
    "res = y_train - y_train_pred\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2);\n",
    "fig.set_figwidth(11);\n",
    "\n",
    "\n",
    "sns.scatterplot(x = y_train, y = res, ax = ax[0]);\n",
    "ax[0].set_title('Plotting residuals against response');\n",
    "ax[0].set_ylabel('Residuals');\n",
    "\n",
    "sns.histplot(res, ax = ax[1], kde = True);\n",
    "ax[1].set_title('Normality of residuals');\n",
    "ax[1].set_xlabel('Residuals');\n",
    "ax[1].set_ylabel('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "params = {'alpha': np.arange(0.001, 10, 0.001)}\n",
    "\n",
    "lasso = GridSearchCV(\n",
    "    estimator = Lasso(),\n",
    "    param_grid = params,\n",
    "    scoring = 'neg_root_mean_squared_error',\n",
    "    cv = 3,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "lasso.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso.best_params_)\n",
    "print(lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lasso = lasso.predict(df_test)\n",
    "print('R-squared for test set:', r2_score(y_test, y_pred_lasso))\n",
    "print('RMSE for test set:', root_mean_squared_error(y_test, y_pred_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the residuals show a clear upward trend, although they appear homoscedastic. We shall now examine the predictors and see which predictors are contributing to this phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['res']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clear trend is seen in the residuals vs `LotArea` plot, which may indicate presence of a polynomial or an exponential relationship. We shall test for exponential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop('res', axis = 1, inplace = True)\n",
    "\n",
    "df_train['exp_LotArea'] = df_train['LotArea'] ** 2\n",
    "df_test['exp_LotArea'] = df_test['LotArea'] ** 2\n",
    "\n",
    "reg0 = LinearRegression()\n",
    "reg0.fit(df_train, y_train)\n",
    "y_pred_reg0 = reg0.predict(df_test)\n",
    "\n",
    "print('Linear Regression scores: \\n')\n",
    "print(f'r-Squared = {r2_score(y_test, y_pred_reg0)}')\n",
    "print(f'MSE = {root_mean_squared_error(y_test, y_pred_reg0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = y_test - y_pred_reg0\n",
    "\n",
    "fig, ax = plt.subplots(1, 2);\n",
    "fig.set_figwidth(11);\n",
    "\n",
    "\n",
    "sns.scatterplot(x = y_test, y = res, ax = ax[0]);\n",
    "ax[0].set_title('Plotting residuals against response');\n",
    "ax[0].set_ylabel('Residuals');\n",
    "\n",
    "sns.histplot(res, ax = ax[1], kde = True);\n",
    "ax[1].set_title('Normality of residuals');\n",
    "ax[1].set_xlabel('Residuals');\n",
    "ax[1].set_ylabel('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QQduz_APD198",
    "zDUuGMEfEBBA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
